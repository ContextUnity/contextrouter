[models]
default = "vertex/gemini-2.5-flash"

# Example: using Groq for intent detection (high speed)
[models.rag.intent]
model = "groq/llama-3.3-70b-versatile"
fallback = ["vertex/gemini-2.5-flash-lite"]
strategy = "fallback"

[models.rag.generation]
model = "vertex/gemini-2.5-flash"
fallback = ["openai/gpt-5.1", "runpod/llama3-8b"]
strategy = "fallback"

# Example: using HF Hub for suggestions
[models.rag.suggestions]
model = "hf-hub/mistralai/Mistral-7B-Instruct-v0.2"
fallback = ["vertex/gemini-2.5-flash-lite"]
strategy = "fallback"

[models.rag.no_results]
model = "vertex/gemini-2.5-flash-lite"
fallback = ["anthropic/claude-haiku-4.5"]
strategy = "fallback"

[llm]
temperature = 0.1
max_output_tokens = 2048

[router]
mode = "agent"

[google_cse]
enabled = true
# api_key and cx are usually provided via environment variables for security
